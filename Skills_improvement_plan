docker plan:
1) docs (file systems (overlay fs))
2) dockerfile (run, entry point, cmd) (layewrs) (try to wright few files)
3) commands (docker run ssh)
4) run docker (with jenkins) connect file transfer network
5) check network betwenn containers
6) docker compose
7) touch docker swarm (probalby use for deploy) --going to die
8) volumes - connect hosts folders
9) try to use diff versdions for jenkinsd ( jobs on host folder)
10)loging (docker logs)

also:
  jenkins try catch
aws
network , users,
monitoring
terraform - easy (1 week)
prometheus
serverless (лямбда)

mionitoring elk efk ()
kuber
nginx tomcat appache

Linux administration techotrack:
(1 hz;2the best ;3 -dns,dig,network TROUBLESHOOTING watch on rewind;4 RPMBUILD,init,systemd;5 theory web srvs(queryies)DB HZ);6 mbr-gpt\raid\lvm\ext4;
7 dns-bind configuration\ntp\pam\ldap auth\dhcp\kickstart;8-9 SALT\backup\bacula;10 smtp\postfix;11 os resource

task: jenkins container with not latest version and workspace on the host;
      own image with dockerfile based on jenkins+ port redirect and host volumes ;
      docker-compose based on my images with few jenkins and nginx's;
      deploy on swarm hosts;
      nginx with proxy on jenkins.hu;
      ERROR: expose port via docker compose
docker notes:
--Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers.
        The use of Linux containers to deploy applications is called containerization.
--docker run as client-server.The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects
        such as images, containers, networks, and volumes. When you use commands such as docker run, the client sends these commands to dockerd,
        which carries them out. The docker command uses the Docker API.
--Docker registry stores Docker images. Docker Hub and Docker Cloud are public registries that anyone can use.
        You can even run your own private registry.
--An image is an executable package that includes everything needed to run an application--the code, a runtime,
        libraries, environment variables, and configuration files.
        An image is a read-only template with instructions for creating a Docker container.
        Often, an image is based on another image, with some additional customization.
        A Docker image is built up from a series of layers. Each layer represents an instruction in the image’s Dockerfile.
        Each layer except the very last one is read-only.Each layer is only a set of differences from the layer before it.
        The layers are stacked on top of each other. When you create a new container, you add a new writable layer on top of the underlying layers.
        This layer is often called the “container layer”. All changes made to the running container, such as writing new files,
        modifying existing files, and deleting files, are written to this thin writable container layer.
--A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI.
        You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.
        Container take by default ip addresses from dhcp pull
--docker image and docker container have their own ID and NAME
--The major difference between a container and an image is the top writable layer.
        All writes to the container that add new or modify existing data are stored in this writable layer.
        When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged.
--A storage driver handles the details about the way these layers interact with each other.
        Different storage drivers are available, which have advantages and disadvantages in different situations.
        Docker uses storage drivers to manage the contents of the image layers and the writable container layer.
        Each storage driver handles the implementation differently, but all drivers use stackable image layers
        and the copy-on-write (CoW) strategy.Copy-on-write is a strategy of sharing and copying files for maximum efficiency.
        If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it,
        it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container)
        the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers.
--Note If you need multiple images to have shared access to the exact same data,
        store this data in a Docker volume and mount it into your containers.
--Ephemeral Containers - A container that immediately exits when finished.
--docker run standalone processes(not daemons cause dont have sysv\systemd utils for it)
--Dockerfile Creates an easy to read, easy to write, user base image.
        each command in Dockerfile build another container(layer) and take up additional space (better combine commands with &&)
        build with dockerfile will use cache if commands in dockerfile was previosly run(cahed steps)
        commands RUN(run command while building image);ENV(set variable with spaces);EXPOSE(port from container exposed but not passed through);
        CMD(run commands after container being run E.G.["command","arg1","arg2"])
--Services allow you to scale containers across multiple Docker daemons, which all work together as a swarm with
        multiple managers and workers. Each member of a swarm is a Docker daemon, and the daemons all communicate using the Docker API.
        A service allows you to define the desired state, such as the number of replicas of the service that must be available at any given time.
        By default, the service is load-balanced across all worker nodes.
        To the consumer, the Docker service appears to be a single application.
        A service only runs one image, but it codifies the way that image runs—what ports it should use,
        how many replicas of the container should run so the service has the capacity it needs, and so on.
        Scaling a service changes the number of container instances running that piece of software,
        assigning more computing resources to the service in the process. "docker service ps".
        to add more services to your stack, you insert them in your Compose file.
        load balancer balance connection across nodes and containers (desribes in compose .yml file)
--Task is a single container running in a service. Tasks are given unique IDs that numerically increment,
        up to the number of replicas you defined in docker-compose.yml.
--A swarm is a group of machines that are running Docker and joined into a cluster.
        After that has happened, you continue to run the Docker commands you’re used to,
        but now they are executed on a cluster by a swarm manager. The machines in a swarm can be physical or virtual.
        After joining a swarm, they are referred to as nodes.
        Swarm managers are the only machines in a swarm that can execute your commands,
        or authorize other machines to join the swarm as workers. Workers are just there to provide capacity and
        do not have the authority to tell any other machine what it can and cannot do.
--docker run # run containers(or command inside)based on images
        #(if run with "-it"you will connect to container but it will be shut after you will exit)
        ps #list containers (-a all that been started before)
        pull #download image(snapshot) from dockerhub to local
        search #search images on dockerhub in terminal
        attach #attach terminal to the process that container is running(after ctrl+c will shut process and container)
        inspect #json about container
        exec #run command(E.G. /bin/bash) inside container (without connecting)exiting will not cause shutdown of the container
          -it #interactivly connect to container (docker exec -it 'Container name' 'command') (cli inside container)
        --name #to add my own name to the running container
        rm\rmi #remove container(rm)\image(rmi) (by ID or name). Can't remove images (without force "-f")if you have containers based on them.
        -d #in background(disconnected)
        -e #execute smth (-d-e The container will continue to run as long as the argument is running, as soon as the argument stops the container will exit.)
        ps -a -q #shows IDs of all not currently not running containers(docker rm `docker ps -a -q`)
        -P #all container ports forwarded to the local host though randomly picked local ports in the 32768 to 65000 range.Ports info at `docker ps`
        -p "local port":"containers port", -//-  #bind local port with container port
        port (CONTAINER) $CONTAINERPORT #Will show the variable of the CONTAINERPORT which is the open ports of the container and the random port links to the local host.
        -v 'path to local dir':'path where to mount inside the container' #to mount local dir in container(changes apply on the fly)
        build -t 'docker hub accout'/imagename . #tag .path to Dockerfile
        stop container stop <hash>           # Gracefully stop the specified container
        container kill <hash>         # Force shutdown of the specified container
        login             # Log in this CLI session using your Docker credentials
        tag <image> username/repository:tag  # Tag <image> for upload to registry
        push username/repository:tag            # Upload tagged image to registry
        stack ls      # List stacks or apps
              deploy -c <composefile> <appname>  # Run the specified Compose file
              rm <appname>   # Tear down an application
        service ls    # List running services associated with an app
                ps <service> # List tasks associated with an app
        swarm init # to enable swarm mode and make your current machine a swarm manager
        swarm leave --force      # Take down a single node swarm from the manager
docker-machine create --driver virtualbox myvm1 # Create a VM
               env myvm1                # View basic information about your node
               ssh myvm1 "docker node ls"         # List the nodes in your swarm
               ssh myvm1 "docker swarm init --advertise-addr <myvm1 ip>" #to add host to swarm as manager (note that you will get TOKEN)



 to install - add docker repo;install docker-engine;add user to docker group; additional docker-compose; additional docker-machineb
What to improve:
  inode;
  

 web servers:
   frontend - user http request, send static or redirect to app server
   backend - processes logic of requests, iteract with DB
   DB -storing data
   nginx asyncronic
   appache syncronic
